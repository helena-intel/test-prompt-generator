"""
Generating test prompts for transformer model testing. Prompts are generated by
cutting off input text (by default Alice In Wonderland) at the specified number of tokens.
"""

import json
import logging
import re
import warnings
from pathlib import Path
from typing import List, Union

from transformers import AutoTokenizer

logging.basicConfig(level=logging.INFO)

# For most models, the tokenizer is the same for variations of the model
# This config allows specifying just "bert" instead of the full model_id, but
# there can always be model implementations that do things differently.
_preset_tokenizers = {
    "bert": "google-bert/bert-base-uncased",
    "blenderbot": "facebook/blenderbot-3B",
    "bloom": "bigscience/bloom-560m",
    "bloomz": "bigscience/bloomz-7b1",
    "chatglm3": "THUDM/chatglm3-6b",
    "falcon": "tiiuae/falcon-7b",
    "gemma": "fxmarty/tiny-random-GemmaForCausalLM",
    "gpt-neox": "EleutherAI/gpt-neox-20b",
    "granite-3": "ibm-granite/granite-3.3-2b-instruct",
    "llama-3": "llmware/llama-3.2-3b-instruct-ov",
    "magicoder": "ise-uiuc/Magicoder-S-DS-6.7B",
    "mistral": "echarlaix/tiny-random-mistral",
    "mpt": "mosaicml/mpt-7b",
    "opt": "facebook/opt-2.7b",
    "phi-2": "microsoft/phi-2",
    "phi-3.5": "microsoft/Phi-3.5-mini-instruct",
    "phi-4": "microsoft/Phi-4-mini-instruct",
    "pythia": "EleutherAI/pythia-1.4b-deduped",
    "qwen1.5": "Qwen/Qwen1.5-7B-Chat",
    "qwen3": "Qwen/Qwen3-0.6B",
    "redpajama": "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
    "roberta": "FacebookAI/roberta-base",
    "starcoder": "bigcode/starcoder2-7b",
    "t5": "google-t5/t5-base",
    "tinyllama": "TinyLlama/TinyLlama-1.1B-Chat-v0.6",
    "vicuna": "lmsys/vicuna-7b-v1.5",
    "zephyr": "HuggingFaceH4/zephyr-7b-beta",
}


def generate_prompt(
    tokenizer_id: str,
    num_tokens: Union[List[int], int],
    prefix: str = None,
    output_file: str = None,
    overwrite=False,
    verbose: bool = False,
    source_text_file: str = None,
    source_text: str = None,
    return_type: str = None,
    no_length_check=False,
):
    ### Validate inputs
    if isinstance(num_tokens, (list, tuple, set)) and len(num_tokens) > 1 and (output_file is None or Path(output_file).suffix != ".jsonl"):
        raise ValueError("When generating multiple prompts, output file should be specified and should be a .jsonl file")
    if isinstance(num_tokens, int):
        num_tokens = [num_tokens]
    if source_text == "":
        source_text = None
    if source_text_file == "":
        source_text_file = None
    if source_text_file is not None and source_text is not None:
        raise ValueError("Only one of `source_text` or `source_text_file` should be provided.")

    if return_type is None:
        return_type = "string" if len(num_tokens) == 1 else "dict"
    if return_type not in ("string", "dict"):
        raise ValueError("return_type should be 'string', 'dict' or None")

    ### Load tokenizer
    if tokenizer_id in _preset_tokenizers:
        tokenizer_id = _preset_tokenizers[tokenizer_id]

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id, trust_remote_code=True)

    ### Generate source text
    if source_text_file is None and source_text is None:
        source_text_file = Path(__file__).parent / "text_files" / "alice.txt"

    if source_text is None:
        source_text = Path(source_text_file).read_text(encoding="utf-8")
        if verbose:
            logging.info(f"Generating prompts from {source_text_file}")

    if prefix is not None:
        prefix_tokens = tokenizer(prefix)["input_ids"]
        prefix_num_tokens = len(prefix_tokens)
        if prefix_num_tokens > min(num_tokens):
            logging.warning(
                f"Requested number of tokens {min(num_tokens)} is smaller than " f"number of prefix tokens: {prefix_num_tokens}"
            )

        source_text = prefix.strip() + " " + source_text

    # Some tokenizers treat "\n\n" at the end of a sentence differently than in the middle;
    # replace consecutive "\n" with 1 to prevent generating an incorrect number of tokens
    source_text = re.sub(r"\n+", "\n", source_text)
    source_text = source_text.replace(r'"', "'")

    # Try to prevent warnings about too many tokens from tokenizing the entire source text
    tokenizer.model_max_length = 12000
    inputs = tokenizer(source_text)

    tokens = inputs["input_ids"]
    total_tokens = len(tokens)
    if max(num_tokens) > total_tokens:
        raise ValueError(f"Cannot generate prompt with {max(num_tokens)} tokens; the source text contains {total_tokens} tokens.")

    prompt_dicts = []
    for i, tok in enumerate(num_tokens):
        num_tokens_from_source = tok
        if tokenizer("hello")["input_ids"][-1] in tokenizer.all_special_ids:
            # The tokenizer adds a special token to the end of the sentence, so if
            # num_tokens == N, we need to get N-1 tokens from the input_text
            num_tokens_from_source -= 1

        ### Generate prompt
        prompt = tokenizer.decode(tokens[:num_tokens_from_source], skip_special_tokens=True)

        ### Postprocessing
        if prompt.endswith(" ") and "t5" in tokenizer_id:
            # If a prompt ends with a space, t5 will drop that from tokenization and the prompt will
            # not have enough tokens. Just increasing num_tokens_from_source doesn't help because
            # then a space and a new word will be added, making a prompt with too many tokens.
            # This solution is not great, but it's simple and works.
            prompt = prompt[:-1] + "."

        if "chatglm" in tokenizer_id:
            # chatglm adds these tokens even when skip_special_tokens=True
            prompt = prompt.replace("[gMASK] sop ", "")

        # Workaround for in-development tokenizers
        regex = r"^(<\|[a-z]+_of_text\|>).*"
        special_tokens = re.findall(regex, prompt)
        for special_token in special_tokens:
            prompt = prompt.replace(special_token, "")

        ### Validation
        prompt_tokens = tokenizer(prompt)
        if len(prompt_tokens["input_ids"]) != tok:
            print("prompt", repr(prompt))
            print("prompt_tokens", prompt_tokens["input_ids"])
            print("source_tokens", tokens[:num_tokens_from_source])
            message = f"Expected {tok} tokens, got {len(prompt_tokens['input_ids'])}. Tokenizer: {tokenizer_id}"
            if no_length_check:
                logging.error(message)
            else:
                raise RuntimeError(message)

        ### Write output file
        prompt_dict = {
            "prompt": prompt,
            "model_id": tokenizer_id,
            "token_size": len(prompt_tokens["input_ids"]),
        }
        prompt_dicts.append(prompt_dict)

    jsonl_result = "\n".join(json.dumps(item) for item in prompt_dicts)

    if output_file is not None:
        output_file = Path(output_file)
        if output_file.exists() and (not overwrite):
            raise FileExistsError(f"{output_file} already exists. Set overwrite to allow overwriting.")
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            if output_file.suffix == ".jsonl":
                f.write(f"{jsonl_result}\n")
            else:
                f.write(prompt)

    if return_type == "string":
        return prompt if len(num_tokens) == 1 else "\n".join(item["prompt"] for item in prompt_dicts)
    else:
        return prompt_dict if len(num_tokens) == 1 else prompt_dicts


def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-t",
        "--tokenizer",
        required=True,
        help="preset tokenizer id, model_id from Hugging Face hub, or path to local directory with tokenizer files. "
        f"Options for presets are: {list(_preset_tokenizers.keys())}",
    )
    parser.add_argument(
        "-n",
        "--num_tokens",
        required=True,
        type=int,
        nargs="*",
        help="Number of tokens the generated prompt should have. To specify multiple token sizes, "
        "use e.g. `-n 16 32` and include `--output_file`",
    )
    parser.add_argument(
        "-p",
        "--prefix",
        required=False,
        help="Optional: prefix that the prompt should start with. Example: 'Translate to Dutch:'",
    )
    parser.add_argument(
        "-o",
        "--output_file",
        required=False,
        help="Optional: Path to store the prompt as .jsonl file",
    )
    parser.add_argument(
        "--overwrite",
        required=False,
        action="store_true",
        help="Overwrite output_file if it already exists.",
    )
    parser.add_argument("-v", "--verbose", action="store_true")
    parser.add_argument(
        "-f",
        "--file",
        required=False,
        help="Optional: path to text file to generate prompts from. Default text_files/alice.txt",
    )
    parser.add_argument("--no-length-check", action="store_true", help="Do not fail if generated prompt has the wrong length.")

    args = parser.parse_args()
    if args.verbose:
        logging.info(f"Command line arguments: {args}")

    result = generate_prompt(
        tokenizer_id=args.tokenizer,
        num_tokens=args.num_tokens,
        prefix=args.prefix,
        output_file=args.output_file,
        overwrite=args.overwrite,
        verbose=args.verbose,
        source_text_file=args.file,
        no_length_check=args.no_length_check,
    )
    return result


if __name__ == "__main__":
    print(main())
